{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well Time Series Analysis with Advanced Analytics\n",
    "\n",
    "This notebook provides comprehensive analysis of well sensor data using advanced outlier detection and visualization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from scipy import stats\n",
    "\n",
    "# Import our custom modules\n",
    "from src.models.well_ts_model import WellLSTM\n",
    "from src.utils.data_processor import TagDataProcessor, prepare_dataloader, WellTimeSeriesDataset\n",
    "from src.utils.notebook_utils import detect_outliers, compare_outlier_methods, detect_time_series_outliers, analyze_seasonal_patterns\n",
    "from src.utils.visualization_utils import (\n",
    "    plot_sensor_analysis,\n",
    "    analyze_data_balance\n",
    ")\n",
    "from src.train import train_model, load_config\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn')\n",
    "%matplotlib inline\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Advanced Data Loading and Preprocessing\n",
    "\n",
    "Enhanced data loading with automatic type inference and timestamp handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "data_path = '../data/tagdata_sample.json'\n",
    "config_path = '../data/tag_config.json'\n",
    "\n",
    "processor = TagDataProcessor(config_path)\n",
    "data = processor.load_and_preprocess_data(data_path)\n",
    "\n",
    "# Display basic statistics\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total samples: {len(data)}\")\n",
    "print(f\"Features: {data.columns.tolist()}\")\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(data.describe())\n",
    "\n",
    "# Check data types and missing values\n",
    "print(\"\\nData Types:\")\n",
    "display(data.dtypes)\n",
    "print(\"\\nMissing Values:\")\n",
    "display(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Advanced Outlier Detection\n",
    "\n",
    "Multiple methods for robust outlier detection in sensor data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Detect outliers for numeric columns\n",
    "numeric_columns = data.select_dtypes(include=[np.number]).columns\n",
    "outliers = detect_outliers(data, numeric_columns)\n",
    "\n",
    "# Plot outlier detection results\n",
    "plt.figure(figsize=(15, 5 * len(numeric_columns)))\n",
    "for i, col in enumerate(numeric_columns, 1):\n",
    "    plt.subplot(len(numeric_columns), 1, i)\n",
    "    \n",
    "    # Plot original data\n",
    "    plt.plot(data.index, data[col], 'b.', label='Normal', alpha=0.5)\n",
    "    \n",
    "    # Plot outliers from different methods\n",
    "    for method, indices in outliers.items():\n",
    "        plt.plot(data.index[indices], data[col].iloc[indices],\n",
    "                 'r.', label=f'{method} outliers', alpha=0.5)\n",
    "    \n",
    "    plt.title(f'Outlier Detection for {col}')\n",
    "    plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each numeric column in your data\n",
    "for column in data.select_dtypes(include=[np.number]).columns:\n",
    "    compare_outlier_methods(data, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Time Series Outlier Detection \n",
    "Rolling Z-Score & Seasonal Decomposition\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "for tag in data['tagalias'].unique():\n",
    "    print(f\"\\nAnalyzing tag: {tag}\")\n",
    "    outliers = detect_time_series_outliers(data, tag)\n",
    "    analyze_seasonal_patterns(data, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enhanced Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot analysis for each tag alias\n",
    "for tag in data['tagalias'].unique():\n",
    "    plot_sensor_analysis(data, tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Balance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Analyze data balance\n",
    "analyze_data_balance(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data):\n",
    "    \"\"\"Create advanced features for time series analysis\"\"\"\n",
    "    # Convert timestamp to datetime if not already\n",
    "    data['timestamp'] = pd.to_datetime(data['timestamp'])\n",
    "    \n",
    "    # Time-based features\n",
    "    data['hour'] = data['timestamp'].dt.hour\n",
    "    data['day_of_week'] = data['timestamp'].dt.dayofweek\n",
    "    data['is_weekend'] = data['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    windows = [5, 10, 20]\n",
    "    for window in windows:\n",
    "        data[f'rolling_mean_{window}'] = data.groupby('tagalias')['value'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).mean()\n",
    "        )\n",
    "        data[f'rolling_std_{window}'] = data.groupby('tagalias')['value'].transform(\n",
    "            lambda x: x.rolling(window=window, min_periods=1).std()\n",
    "        )\n",
    "    \n",
    "    # Lag features\n",
    "    lags = [1, 2, 3, 5, 10]\n",
    "    for lag in lags:\n",
    "        data[f'lag_{lag}'] = data.groupby('tagalias')['value'].shift(lag)\n",
    "    \n",
    "    # Difference features\n",
    "    data['diff_1'] = data.groupby('tagalias')['value'].diff()\n",
    "    data['diff_2'] = data.groupby('tagalias')['value'].diff(2)\n",
    "    \n",
    "    # Percentage change\n",
    "    data['pct_change'] = data.groupby('tagalias')['value'].pct_change()\n",
    "    \n",
    "    # Fill NaN values created by transformations\n",
    "    data = data.fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Engineer features\n",
    "data_engineered = engineer_features(data.copy())\n",
    "\n",
    "# Display correlation heatmap of engineered features\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(data_engineered.select_dtypes(include=[np.number]).corr(),\n",
    "            annot=True, fmt='.2f', cmap='coolwarm')\n",
    "plt.title('Feature Correlation Heatmap')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LSTM Model Architecture and Training\n",
    "\n",
    "Implementation of an LSTM model with attention mechanism for time series prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLSTM(nn.Module):\n",
    "    \"\"\"LSTM model with attention mechanism for time series prediction\"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Output layers\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Initialize hidden state if not provided\n",
    "        if hidden is None:\n",
    "            h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(x.device)\n",
    "            hidden = (h0, c0)\n",
    "        \n",
    "        # LSTM forward pass\n",
    "        lstm_out, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc(context_vector)\n",
    "        \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data_engineered, sequence_length=24, prediction_horizon=1, train_ratio=0.8):\n",
    "    \"\"\"Prepare data for training\"\"\"\n",
    "    # Select features for training\n",
    "    feature_columns = [\n",
    "        'value', 'hour', 'day_of_week', 'is_weekend',\n",
    "        'rolling_mean_5', 'rolling_std_5',\n",
    "        'lag_1', 'lag_2', 'diff_1', 'pct_change'\n",
    "    ]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data_engineered[feature_columns])\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=feature_columns)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_df) - sequence_length - prediction_horizon + 1):\n",
    "        X.append(scaled_df.iloc[i:i+sequence_length].values)\n",
    "        y.append(scaled_df.iloc[i+sequence_length+prediction_horizon-1]['value'])\n",
    "    \n",
    "    X = torch.FloatTensor(np.array(X))\n",
    "    y = torch.FloatTensor(np.array(y))\n",
    "    \n",
    "    # Split into train and validation sets\n",
    "    train_size = int(len(X) * train_ratio)\n",
    "    X_train, X_val = X[:train_size], X[train_size:]\n",
    "    y_train, y_val = y[:train_size], y[train_size:]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, scaler\n",
    "\n",
    "# Prepare data for training\n",
    "train_loader, val_loader, scaler = prepare_training_data(data_engineered)\n",
    "\n",
    "# Model parameters\n",
    "input_size = 10  # Number of input features\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 50\n",
    "\n",
    "# Initialize model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = AttentionLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
    "    \"\"\"Train the model\"\"\"\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                output, _ = model(X_batch)\n",
    "                loss = criterion(output.squeeze(), y_batch)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Calculate average losses\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'scaler': scaler\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}')\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train the model\n",
    "train_losses, val_losses = train_model(\n",
    "    model, train_loader, val_loader,\n",
    "    criterion, optimizer, num_epochs, device\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Model Training History')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Prediction Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_predictions(model, val_loader, scaler, device):\n",
    "    \"\"\"Evaluate model predictions\"\"\"\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    attention_weights_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            \n",
    "            output, attention_weights = model(X_batch)\n",
    "            \n",
    "            predictions.extend(output.cpu().numpy())\n",
    "            actuals.extend(y_batch.numpy())\n",
    "            attention_weights_list.extend(attention_weights.cpu().numpy())\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    attention_weights_list = np.array(attention_weights_list)\n",
    "    \n",
    "    # Inverse transform predictions and actuals\n",
    "    predictions_orig = scaler.inverse_transform(predictions)\n",
    "    actuals_orig = scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals_orig, predictions_orig)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals_orig, predictions_orig)\n",
    "    r2 = r2_score(actuals_orig, predictions_orig)\n",
    "    \n",
    "    print(f'Mean Squared Error: {mse:.4f}')\n",
    "    print(f'Root Mean Squared Error: {rmse:.4f}')\n",
    "    print(f'Mean Absolute Error: {mae:.4f}')\n",
    "    print(f'RÂ² Score: {r2:.4f}')\n",
    "    \n",
    "    # Plot predictions vs actuals\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(actuals_orig, label='Actual')\n",
    "    plt.plot(predictions_orig, label='Predicted')\n",
    "    plt.title('Actual vs Predicted Values')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot attention weights heatmap\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(attention_weights_list[0], cmap='viridis')\n",
    "    plt.title('Attention Weights Heatmap')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Features')\n",
    "    plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "evaluate_predictions(model, val_loader, scaler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Advanced Training Utilities\n",
    "\n",
    "Additional utilities for improved model training and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    def __init__(self, patience=7, min_delta=0, verbose=False):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            \n",
    "def train_model_advanced(model, train_loader, val_loader, criterion, optimizer, \n",
    "                        num_epochs, device, patience=7, scheduler_factor=0.1):\n",
    "    \"\"\"Advanced training with early stopping and learning rate scheduling\"\"\"\n",
    "    early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=scheduler_factor, patience=patience//2, verbose=True\n",
    "    )\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_batch = y_batch.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output, _ = model(X_batch)\n",
    "            loss = criterion(output.squeeze(), y_batch)\n",
    "            \n",
    "            loss.backward()\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_predictions = []\n",
    "        val_actuals = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "                \n",
    "                output, _ = model(X_batch)\n",
    "                loss = criterion(output.squeeze(), y_batch)\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                val_predictions.extend(output.cpu().numpy())\n",
    "                val_actuals.extend(y_batch.cpu().numpy())\n",
    "        \n",
    "        # Calculate average losses and metrics\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_actuals = np.array(val_actuals)\n",
    "        val_mse = mean_squared_error(val_actuals, val_predictions)\n",
    "        val_rmse = np.sqrt(val_mse)\n",
    "        val_mae = mean_absolute_error(val_actuals, val_predictions)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "                'val_metrics': {\n",
    "                    'mse': val_mse,\n",
    "                    'rmse': val_rmse,\n",
    "                    'mae': val_mae\n",
    "                },\n",
    "                'scaler': scaler\n",
    "            }, 'best_model.pth')\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}]')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "        print(f'Val RMSE: {val_rmse:.4f}, Val MAE: {val_mae:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(val_loss)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "    \n",
    "    return train_losses, val_losses\n",
    "\n",
    "# Train with advanced features\n",
    "train_losses, val_losses = train_model_advanced(\n",
    "    model, train_loader, val_loader,\n",
    "    criterion, optimizer, num_epochs, device\n",
    ")\n",
    "\n",
    "# Plot training history with moving averages\n",
    "def plot_training_history(train_losses, val_losses, window=5):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Calculate moving averages\n",
    "    train_ma = pd.Series(train_losses).rolling(window=window).mean()\n",
    "    val_ma = pd.Series(val_losses).rolling(window=window).mean()\n",
    "    \n",
    "    plt.plot(train_losses, label='Training Loss', alpha=0.3)\n",
    "    plt.plot(val_losses, label='Validation Loss', alpha=0.3)\n",
    "    plt.plot(train_ma, label=f'Training Loss ({window}-epoch MA)', linewidth=2)\n",
    "    plt.plot(val_ma, label=f'Validation Loss ({window}-epoch MA)', linewidth=2)\n",
    "    \n",
    "    plt.title('Model Training History with Moving Averages')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data_engineered, k=5, sequence_length=24, prediction_horizon=1):\n",
    "    \"\"\"Perform k-fold cross-validation\"\"\"\n",
    "    # Prepare data\n",
    "    feature_columns = [\n",
    "        'value', 'hour', 'day_of_week', 'is_weekend',\n",
    "        'rolling_mean_5', 'rolling_std_5',\n",
    "        'lag_1', 'lag_2', 'diff_1', 'pct_change'\n",
    "    ]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(data_engineered[feature_columns])\n",
    "    scaled_df = pd.DataFrame(scaled_data, columns=feature_columns)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = [], []\n",
    "    for i in range(len(scaled_df) - sequence_length - prediction_horizon + 1):\n",
    "        X.append(scaled_df.iloc[i:i+sequence_length].values)\n",
    "        y.append(scaled_df.iloc[i+sequence_length+prediction_horizon-1]['value'])\n",
    "    \n",
    "    X = torch.FloatTensor(np.array(X))\n",
    "    y = torch.FloatTensor(np.array(y))\n",
    "    \n",
    "    # Prepare k-fold cross-validation\n",
    "    fold_size = len(X) // k\n",
    "    metrics = []\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(f'\\nFold {fold+1}/{k}')\n",
    "        \n",
    "        # Split data\n",
    "        val_start = fold * fold_size\n",
    "        val_end = (fold + 1) * fold_size\n",
    "        \n",
    "        X_train = torch.cat([X[:val_start], X[val_end:]])\n",
    "        y_train = torch.cat([y[:val_start], y[val_end:]])\n",
    "        X_val = X[val_start:val_end]\n",
    "        y_val = y[val_start:val_end]\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = TensorDataset(X_train, y_train)\n",
    "        val_dataset = TensorDataset(X_val, y_val)\n",
    "        \n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "        \n",
    "        # Initialize model\n",
    "        model = AttentionLSTM(input_size, hidden_size, num_layers, output_size).to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Train model\n",
    "        train_losses, val_losses = train_model_advanced(\n",
    "            model, train_loader, val_loader,\n",
    "            criterion, optimizer, num_epochs, device\n",
    "        )\n",
    "        \n",
    "        # Evaluate fold\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, _ in val_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                output, _ = model(X_batch)\n",
    "                val_predictions.extend(output.cpu().numpy())\n",
    "        \n",
    "        val_predictions = np.array(val_predictions)\n",
    "        val_actuals = y_val.numpy()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fold_metrics = {\n",
    "            'mse': mean_squared_error(val_actuals, val_predictions),\n",
    "            'rmse': np.sqrt(mean_squared_error(val_actuals, val_predictions)),\n",
    "            'mae': mean_absolute_error(val_actuals, val_predictions),\n",
    "            'r2': r2_score(val_actuals, val_predictions)\n",
    "        }\n",
    "        \n",
    "        metrics.append(fold_metrics)\n",
    "        print(f'Fold {fold+1} Metrics:')\n",
    "        for metric, value in fold_metrics.items():\n",
    "            print(f'{metric.upper()}: {value:.4f}')\n",
    "    \n",
    "    # Calculate and print average metrics\n",
    "    print('\\nAverage Metrics across all folds:')\n",
    "    avg_metrics = {}\n",
    "    for metric in metrics[0].keys():\n",
    "        avg_metrics[metric] = np.mean([fold[metric] for fold in metrics])\n",
    "        print(f'{metric.upper()}: {avg_metrics[metric]:.4f}')\n",
    "    \n",
    "    return metrics, avg_metrics\n",
    "\n",
    "# Perform k-fold cross-validation\n",
    "fold_metrics, avg_metrics = k_fold_cross_validation(data_engineered)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
